# Local AI Integration for Log Management

‡∏£‡∏∞‡∏ö‡∏ö AI Assistant ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö **Local AI models** ‡∏î‡πâ‡∏ß‡∏¢ **Ollama** ‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏≤‡∏Å OpenAI ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

## üéØ ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á Local AI

### ‚úÖ **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ**
- **‡∏ü‡∏£‡∏µ 100%** - ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢ API
- **‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢** - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö
- **Offline** - ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï
- **‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î Usage** - ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î
- **Privacy** - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• log ‡πÑ‡∏°‡πà‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°

### ‚ö†Ô∏è **‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î**
- **‡πÉ‡∏ä‡πâ Resource** - CPU, RAM, Storage ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á
- **‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤** - ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö hardware
- **‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤** - ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏î‡∏µ‡πÄ‡∏ó‡πà‡∏≤ GPT-4
- **‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Model** - ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡∏∞‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà

---

## üöÄ ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤

### 1. ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Ollama Service

```bash
# Start Ollama service
docker-compose up -d ollama

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
docker-compose ps ollama
```

### 2. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î AI Models

```bash
# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Llama 3.2 (3B parameters - ‡πÄ‡∏£‡πá‡∏ß, ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ)
docker exec -it auth_logs_ollama ollama pull llama3.2:3b

# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Llama 3.2 (1B parameters - ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å, ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏û‡∏≠‡πÉ‡∏ä‡πâ)
docker exec -it auth_logs_ollama ollama pull llama3.2:1b

# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Llama 3.1 (8B parameters - ‡∏ä‡πâ‡∏≤, ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á)
docker exec -it auth_logs_ollama ollama pull llama3.1:8b
```

### 3. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Environment Variables

```bash
# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ Local AI
export AI_PROVIDER=local
export LOCAL_MODEL=llama3.2:3b

# ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏ô docker-compose.yml
environment:
  - AI_PROVIDER=local
  - LOCAL_MODEL=llama3.2:3b
```

---

## üîß ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `AI_PROVIDER` | `openai` | `openai`, `local`, ‡∏´‡∏£‡∏∑‡∏≠ `auto` |
| `OLLAMA_HOST` | `ollama` | Hostname ‡∏Ç‡∏≠‡∏á Ollama service |
| `OLLAMA_PORT` | `11434` | Port ‡∏Ç‡∏≠‡∏á Ollama API |
| `LOCAL_MODEL` | `llama3.2:3b` | Model ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ |

### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á docker-compose.yml

```yaml
services:
  backend:
    environment:
      - AI_PROVIDER=local          # ‡πÉ‡∏ä‡πâ Local AI
      - OLLAMA_HOST=ollama
      - OLLAMA_PORT=11434
      - LOCAL_MODEL=llama3.2:3b
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}  # backup
    depends_on:
      - ollama

  ollama:
    image: ollama/ollama:latest
    container_name: auth_logs_ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - app-network
```

---

## ü§ñ Models ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥

### **Small Models (‡πÄ‡∏£‡πá‡∏ß, resource ‡∏ô‡πâ‡∏≠‡∏¢)**
```bash
# Llama 3.2 1B (‡∏Ç‡∏ô‡∏≤‡∏î ~1.3GB)
ollama pull llama3.2:1b

# Gemma 2B (‡∏Ç‡∏ô‡∏≤‡∏î ~1.6GB)  
ollama pull gemma:2b

# Phi-3 Mini (‡∏Ç‡∏ô‡∏≤‡∏î ~2.3GB)
ollama pull phi3:mini
```

### **Medium Models (‡∏™‡∏°‡∏î‡∏∏‡∏•)**
```bash
# Llama 3.2 3B (‡∏Ç‡∏ô‡∏≤‡∏î ~2.0GB) - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
ollama pull llama3.2:3b

# Mistral 7B (‡∏Ç‡∏ô‡∏≤‡∏î ~4.1GB)
ollama pull mistral:7b

# Code Llama 7B (‡∏Ç‡∏ô‡∏≤‡∏î ~3.8GB)
ollama pull codellama:7b
```

### **Large Models (‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á, ‡∏ä‡πâ‡∏≤)**
```bash
# Llama 3.1 8B (‡∏Ç‡∏ô‡∏≤‡∏î ~4.7GB)
ollama pull llama3.1:8b

# Llama 3.1 70B (‡∏Ç‡∏ô‡∏≤‡∏î ~40GB)
ollama pull llama3.1:70b
```

---

## üìä ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û

### Benchmark Results

| Model | Size | Speed | Quality | Memory | Best For |
|-------|------|--------|---------|---------|----------|
| llama3.2:1b | 1.3GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | 2-4GB | Quick responses |
| llama3.2:3b | 2.0GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | 4-6GB | **Recommended** |
| mistral:7b | 4.1GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 8-12GB | High quality |
| llama3.1:8b | 4.7GB | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 12-16GB | Best quality |

### Response Time Comparison

```
Task: "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå failed login attempts"

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Provider        ‚îÇ Time     ‚îÇ Quality        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ OpenAI GPT-4    ‚îÇ 2-5s     ‚îÇ ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê    ‚îÇ
‚îÇ OpenAI GPT-3.5  ‚îÇ 1-3s     ‚îÇ ‚≠ê‚≠ê‚≠ê‚≠ê     ‚îÇ
‚îÇ Llama 3.2:1b    ‚îÇ 3-8s     ‚îÇ ‚≠ê‚≠ê‚≠ê       ‚îÇ
‚îÇ Llama 3.2:3b    ‚îÇ 5-15s    ‚îÇ ‚≠ê‚≠ê‚≠ê‚≠ê     ‚îÇ
‚îÇ Mistral 7b      ‚îÇ 10-30s   ‚îÇ ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîÑ Hybrid AI System

‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö **Hybrid AI** ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å provider ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥:

### Auto Selection Priority
1. **Local AI** (‡∏ñ‡πâ‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)
2. **OpenAI** (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ API key)
3. **Error** (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ provider ‡πÉ‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏°)

### Manual Selection
```bash
# ‡πÉ‡∏ä‡πâ Local AI ‡πÄ‡∏™‡∏°‡∏≠
export AI_PROVIDER=local

# ‡πÉ‡∏ä‡πâ OpenAI ‡πÄ‡∏™‡∏°‡∏≠  
export AI_PROVIDER=openai

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (default)
export AI_PROVIDER=auto
```

---

## üß™ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö

### ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Local AI

```bash
cd backend
python app/test_local_ai.py
```

### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
```
ü§ñ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö Local AI Integration

üîó ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama...
  ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠: ‚úÖ ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
  Ollama URL: http://ollama:11434
  Default Model: llama3.2:3b

üì¶ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Models...
  Models ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà: 2 models
    - llama3.2:3b
    - llama3.2:1b
  Default model (llama3.2:3b): ‚úÖ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

üí¨ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°...
  ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏ú‡∏°‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏∞‡∏ö‡∏ö Log Management...
  ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: 5.43 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ

‚úÖ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î! Local AI ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
```

---

## üíª ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

### 1. API Endpoints

‡∏ó‡∏∏‡∏Å endpoint ‡πÄ‡∏î‡∏¥‡∏°‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ Local AI ‡πÅ‡∏ó‡∏ô OpenAI:

```bash
# ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡πÉ‡∏ä‡πâ Local AI)
curl -X POST "http://localhost/api/ai/ask" \
  -H "Content-Type: application/json" \
  -d '{"question": "‡∏°‡∏µ failed login ‡∏Å‡∏µ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ?"}'

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ AI providers
curl "http://localhost/api/ai/status"
```

### 2. Frontend

‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á provider ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:
- **Local AI (llama3.2:3b)** - ‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß
- **OpenAI (gpt-3.5-turbo)** - ‡∏™‡∏µ‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥‡∏ô

### 3. Response Format

Response ‡∏à‡∏∞‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• provider ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:

```json
{
  "success": true,
  "result": {
    "answer": "‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏µ failed login attempts ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 23 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á...",
    "type": "answer",
    "provider_used": "local"
  }
}
```

---

## üîß ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢

#### 1. **Ollama ‡πÑ‡∏°‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô**
```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö logs
docker-compose logs ollama

# Restart service
docker-compose restart ollama
```

#### 2. **Model ‡πÑ‡∏°‡πà‡∏°‡∏µ**
```bash
# ‡∏î‡∏π models ‡∏ó‡∏µ‡πà‡∏°‡∏µ
docker exec -it auth_logs_ollama ollama list

# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î model
docker exec -it auth_logs_ollama ollama pull llama3.2:3b
```

#### 3. **Memory ‡πÑ‡∏°‡πà‡∏û‡∏≠**
```bash
# ‡πÉ‡∏ä‡πâ model ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤
export LOCAL_MODEL=llama3.2:1b

# ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏° swap
sudo fallocate -l 4G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

#### 4. **‡∏ä‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ**
```bash
# ‡πÉ‡∏ä‡πâ model ‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤
export LOCAL_MODEL=llama3.2:1b

# ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô OpenAI
export AI_PROVIDER=openai
```

#### 5. **GPU Support** (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ NVIDIA GPU)
```yaml
# ‡πÉ‡∏ô docker-compose.yml
ollama:
  image: ollama/ollama:latest
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
```

---

## üìà ‡∏Å‡∏≤‡∏£ Monitoring

### 1. ‡∏î‡∏π Resource Usage

```bash
# CPU ‡πÅ‡∏•‡∏∞ Memory usage
docker stats auth_logs_ollama

# Disk usage
docker exec -it auth_logs_ollama du -sh /root/.ollama
```

### 2. ‡∏î‡∏π Model Performance

```bash
# ‡∏î‡∏π model ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
docker exec -it auth_logs_ollama ollama ps

# ‡∏î‡∏π model details
docker exec -it auth_logs_ollama ollama show llama3.2:3b
```

### 3. ‡∏î‡∏π API Logs

```bash
# ‡∏î‡∏π AI Assistant logs
docker-compose logs backend | grep "AI Assistant"

# ‡∏î‡∏π performance metrics
curl "http://localhost/api/ai/status"
```

---

## üîÑ ‡∏Å‡∏≤‡∏£ Migration

### ‡∏à‡∏≤‡∏Å OpenAI ‡πÄ‡∏õ‡πá‡∏ô Local AI

1. **‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Ollama**
   ```bash
   docker-compose up -d ollama
   ```

2. **‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Model**
   ```bash
   docker exec -it auth_logs_ollama ollama pull llama3.2:3b
   ```

3. **‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Environment**
   ```bash
   export AI_PROVIDER=local
   docker-compose restart backend
   ```

4. **‡∏ó‡∏î‡∏™‡∏≠‡∏ö**
   ```bash
   python app/test_local_ai.py
   ```

### ‡∏à‡∏≤‡∏Å Local AI ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô OpenAI

```bash
export AI_PROVIDER=openai
export OPENAI_API_KEY=your-api-key
docker-compose restart backend
```

---

## üéõÔ∏è ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á Advanced

### 1. Custom Model Parameters

```python
# ‡πÉ‡∏ô local_ai.py
payload = {
    "model": model,
    "prompt": prompt,
    "stream": False,
    "options": {
        "temperature": 0.3,      # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå (0-1)
        "top_p": 0.9,           # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ (0-1)
        "top_k": 40,            # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô candidates
        "num_ctx": 4096,        # Context window
        "repeat_penalty": 1.1,   # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥
    }
}
```

### 2. Multiple Models

```bash
# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏´‡∏•‡∏≤‡∏¢ models ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≤‡∏á‡πÜ
ollama pull llama3.2:1b    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡πÄ‡∏£‡πá‡∏ß
ollama pull llama3.2:3b    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ  
ollama pull codellama:7b   # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SQL generation
```

### 3. Load Balancing

```python
# ‡πÉ‡∏ô ai_assistant.py
async def _call_ai_with_fallback(self, prompt: str):
    try:
        # ‡∏•‡∏≠‡∏á Local AI ‡∏Å‡πà‡∏≠‡∏ô
        return await self.hybrid_client.generate_response(prompt, provider='local')
    except:
        # ‡∏ñ‡πâ‡∏≤‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡πÉ‡∏ä‡πâ OpenAI
        return await self.hybrid_client.generate_response(prompt, provider='openai')
```

---

## üí∞ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô

### Cost Comparison (monthly)

```
Scenario: 1000 AI requests/day

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Provider        ‚îÇ Cost/Month    ‚îÇ Quality      ‚îÇ Privacy       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ OpenAI GPT-4    ‚îÇ $150-300      ‚îÇ ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê   ‚îÇ ‚≠ê‚≠ê          ‚îÇ
‚îÇ OpenAI GPT-3.5  ‚îÇ $20-50        ‚îÇ ‚≠ê‚≠ê‚≠ê‚≠ê     ‚îÇ ‚≠ê‚≠ê          ‚îÇ
‚îÇ Local AI        ‚îÇ $0            ‚îÇ ‚≠ê‚≠ê‚≠ê‚≠ê     ‚îÇ ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê    ‚îÇ
‚îÇ Hardware Cost   ‚îÇ $20-50/month  ‚îÇ -            ‚îÇ -             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Resource Requirements

```
Minimum Requirements:
- CPU: 4 cores
- RAM: 8GB
- Storage: 10GB

Recommended:
- CPU: 8 cores  
- RAM: 16GB
- Storage: 50GB
- GPU: Optional (NVIDIA)
```

---

## üöÄ Roadmap

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
- [ ] Support ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Vision models (‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û)
- [ ] Fine-tuning ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö log analysis
- [ ] Model quantization (‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î)
- [ ] Multi-modal AI (text + data)
- [ ] Automated model updates
- [ ] Performance optimization

---

## üìö Resources ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°

### Documentation
- [Ollama Official](https://ollama.ai/)
- [Llama Models](https://ai.meta.com/llama/)
- [Model Hub](https://ollama.ai/library)

### Community
- [Ollama Discord](https://discord.gg/ollama)
- [Reddit r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/)

---

## üéâ ‡∏™‡∏£‡∏∏‡∏õ Local AI Integration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Log Management

‡∏ú‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏° **Local AI models** ‡∏î‡πâ‡∏ß‡∏¢ **Ollama** ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ AI ‡πÅ‡∏ö‡∏ö‡∏ü‡∏£‡∏µ‡πÅ‡∏•‡∏∞‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÅ‡∏ó‡∏ô OpenAI

### ‚úÖ **‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß:**

#### üîß **Backend Infrastructure:**
1. **Ollama Service** ‡πÉ‡∏ô `docker-compose.yml`
   - Container ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ô Local AI models
   - Volume ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö models
   - Network configuration

2. **Local AI Client** (`backend/app/services/local_ai.py`)
   - `LocalAIClient` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama
   - `HybridAIClient` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å provider ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
   - Model management ‡πÅ‡∏•‡∏∞ auto-download

3. **AI Assistant Integration**
   - ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï `ai_assistant.py` ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Hybrid AI
   - Support ‡∏ó‡∏±‡πâ‡∏á OpenAI ‡πÅ‡∏•‡∏∞ Local AI
   - Auto fallback ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á providers

#### ‚öôÔ∏è **Configuration & Settings:**
4. **Environment Variables**
   - `AI_PROVIDER`: `openai`, `local`, ‡∏´‡∏£‡∏∑‡∏≠ `auto`
   - `OLLAMA_HOST`, `OLLAMA_PORT`, `LOCAL_MODEL`
   - Auto-selection priority: Local > OpenAI

5. **Frontend Updates**
   - ‡πÅ‡∏™‡∏î‡∏á provider ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö
   - Local AI (‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß), OpenAI (‡∏™‡∏µ‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥‡∏ô)
   - Real-time status checking

#### üß™ **Testing & Documentation:**
6. **Comprehensive Testing** (`backend/app/test_local_ai.py`)
   - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Ollama connection
   - Model management tests
   - Performance comparison
   - Integration tests

7. **Complete Documentation** (`README_LOCAL_AI.md`)
   - Installation guide
   - Model recommendations
   - Troubleshooting
   - Cost comparison

### üöÄ **‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:**

#### **1. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Local AI:**
```bash
# 1. Start Ollama service
docker-compose up -d ollama

# 2. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î model (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
docker exec -it auth_logs_ollama ollama pull llama3.2:3b

# 3. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ Local AI
export AI_PROVIDER=local
docker-compose restart backend

# 4. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö
cd backend
python app/test_local_ai.py
```

#### **2. Models ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:**
- **llama3.2:1b** (1.3GB) - ‡πÄ‡∏£‡πá‡∏ß, resource ‡∏ô‡πâ‡∏≠‡∏¢
- **llama3.2:3b** (2.0GB) - **‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥** ‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏î‡∏µ
- **mistral:7b** (4.1GB) - ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á

#### **3. ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Provider:**
```bash
<code_block_to_apply_changes_from>
```

### üí∞ **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á Local AI:**

#### ‚úÖ **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:**
- **‡∏ü‡∏£‡∏µ 100%** - ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ API
- **‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢** - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å‡∏ô‡∏≠‡∏Å‡∏£‡∏∞‡∏ö‡∏ö
- **Offline** - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï
- **Privacy** - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• log ‡πÑ‡∏°‡πà‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°
- **‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î Usage** - ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î

#### ‚ö†Ô∏è **‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î:**
- **‡πÉ‡∏ä‡πâ Resource** - CPU, RAM, Storage ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á
- **‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤** - 5-15s vs 1-3s (OpenAI)
- **‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤** - ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö GPT-4

### üìä **‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:**

| Provider | Cost/Month | Speed | Quality | Privacy | Offline |
|----------|------------|--------|---------|---------|---------|
| OpenAI GPT-4 | $150-300 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚ùå |
| OpenAI GPT-3.5 | $20-50 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚ùå |
| **Local AI** | **$0** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ |

### üîÑ **Hybrid AI System:**

‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞**‡πÄ‡∏•‡∏∑‡∏≠‡∏Å provider ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥**:
1. ‡∏•‡∏≠‡∏á **Local AI** ‡∏Å‡πà‡∏≠‡∏ô (‡∏ñ‡πâ‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°)
2. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏ä‡πâ **OpenAI** (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ API key)
3. ‡πÅ‡∏™‡∏î‡∏á error ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ provider ‡πÉ‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏°

### üéØ **Use Cases ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞:**

#### **‡πÉ‡∏ä‡πâ Local AI ‡πÄ‡∏°‡∏∑‡πà‡∏≠:**
- ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢
- ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà sensitive
- ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏™‡∏π‡∏á
- ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô on-premise

#### **‡πÉ‡∏ä‡πâ OpenAI ‡πÄ‡∏°‡∏∑‡πà‡∏≠:**
- ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
- ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
- Resource ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≥‡∏Å‡∏±‡∏î
- ‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏°‡∏≤‡∏Å

### üìã **‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå:**

```bash
# ‡∏î‡∏π models ‡∏ó‡∏µ‡πà‡∏°‡∏µ
docker exec -it auth_logs_ollama ollama list

# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î model ‡πÉ‡∏´‡∏°‡πà
docker exec -it auth_logs_ollama ollama pull llama3.2:1b

# ‡∏î‡∏π resource usage
docker stats auth_logs_ollama

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ AI
curl http://localhost/api/ai/status
```

**‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ AI Assistant ‡∏ó‡∏µ‡πà‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡πÅ‡∏•‡πâ‡∏ß!** ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á:
- üí∞ **OpenAI** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á
- üÜì **Local AI** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î
- üîÑ **Auto** ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏á

‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡πÅ‡∏ö‡∏ö ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏£‡∏±‡∏ö! üöÄ 