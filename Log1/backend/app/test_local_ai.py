#!/usr/bin/env python3
"""
Test Local AI Integration
‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Local AI models ‡∏Å‡∏±‡∏ö Ollama
"""
import sys
import os
import asyncio
import time
from datetime import datetime

# ‡πÄ‡∏û‡∏¥‡πà‡∏° path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö import module
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.services.local_ai import LocalAIClient, HybridAIClient
from app.core.config import settings

def test_ollama_connection():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama"""
    print("üîó ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama...")
    
    client = LocalAIClient()
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
    connection_ok = client.test_connection()
    print(f"  ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠: {'‚úÖ ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à' if connection_ok else '‚ùå ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß'}")
    
    if connection_ok:
        print(f"  Ollama URL: {client.base_url}")
        print(f"  Default Model: {client.local_model}")
    
    return connection_ok

def test_model_management():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ models"""
    print("\nüì¶ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Models...")
    
    client = LocalAIClient()
    
    # ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ models
    models = client.list_models()
    print(f"  Models ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà: {len(models)} models")
    for model in models:
        print(f"    - {model}")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö default model
    default_model = client.local_model
    model_available = client.ensure_model_available(default_model)
    print(f"  Default model ({default_model}): {'‚úÖ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô' if model_available else '‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°'}")
    
    return len(models) > 0

async def test_text_generation():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
    print("\nüí¨ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°...")
    
    client = LocalAIClient()
    
    try:
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö prompt ‡∏á‡πà‡∏≤‡∏¢‡πÜ
        prompt = "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ ‡∏ú‡∏°‡∏Ñ‡∏∑‡∏≠ AI Assistant ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö Log Management"
        start_time = time.time()
        
        print(f"  Prompt: {prompt}")
        print("  ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö...")
        
        response = await client.generate_response(prompt)
        duration = time.time() - start_time
        
        print(f"  ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {response[:100]}{'...' if len(response) > 100 else ''}")
        print(f"  ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {duration:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
        
        return len(response) > 0
        
    except Exception as e:
        print(f"  ‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        return False

async def test_chat_completion():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö chat completion API"""
    print("\nüí≠ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Chat Completion...")
    
    client = LocalAIClient()
    
    try:
        messages = [
            {"role": "system", "content": "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI Assistant ‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô cybersecurity"},
            {"role": "user", "content": "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå log ‡∏ó‡∏µ‡πà‡∏°‡∏µ failed login ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å"}
        ]
        
        start_time = time.time()
        response = await client.chat_completion(messages)
        duration = time.time() - start_time
        
        print(f"  ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {response[:100]}{'...' if len(response) > 100 else ''}")
        print(f"  ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {duration:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
        
        return len(response) > 0
        
    except Exception as e:
        print(f"  ‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        return False

def test_hybrid_ai_client():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö Hybrid AI Client"""
    print("\nüîÄ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Hybrid AI Client...")
    
    hybrid = HybridAIClient()
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö providers ‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    providers = hybrid.get_available_providers()
    print(f"  Providers ‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô: {providers}")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å provider ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    recommended = hybrid.auto_select_provider()
    print(f"  Provider ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: {recommended}")
    
    # ‡∏î‡∏∂‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    status = hybrid.get_status()
    print(f"  ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ OpenAI: {'‚úÖ' if status['openai']['available'] else '‚ùå'}")
    print(f"  ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Local AI: {'‚úÖ' if status['local']['available'] else '‚ùå'}")
    
    return len(providers) > 0

async def test_hybrid_generation():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ Hybrid Client"""
    print("\nü§ñ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Hybrid Text Generation...")
    
    hybrid = HybridAIClient()
    
    try:
        prompt = "‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå security log ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö"
        system_prompt = "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô Security Analyst ‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç"
        
        start_time = time.time()
        response = await hybrid.generate_response(prompt, system_prompt)
        duration = time.time() - start_time
        
        provider_used = hybrid.auto_select_provider()
        
        print(f"  Provider ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {provider_used}")
        print(f"  ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {response[:150]}{'...' if len(response) > 150 else ''}")
        print(f"  ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {duration:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
        
        return len(response) > 0
        
    except Exception as e:
        print(f"  ‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        return False

def test_token_estimation():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì tokens"""
    print("\nüî¢ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì Tokens...")
    
    client = LocalAIClient()
    
    test_texts = [
        "Hello world",
        "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢",
        "This is a longer text that should have more tokens than the previous examples."
    ]
    
    for text in test_texts:
        tokens = client.estimate_tokens(text)
        print(f"  '{text[:30]}...': {tokens} tokens")
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    long_text = "This is a very long text. " * 100
    original_tokens = client.estimate_tokens(long_text)
    truncated = client.truncate_text(long_text, max_tokens=50)
    truncated_tokens = client.estimate_tokens(truncated)
    
    print(f"  ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß: {original_tokens} tokens -> ‡∏ï‡∏±‡∏î‡πÄ‡∏´‡∏•‡∏∑‡∏≠: {truncated_tokens} tokens")
    
    return True

async def test_performance_comparison():
    """‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û Local vs OpenAI"""
    print("\n‚ö° ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û...")
    
    hybrid = HybridAIClient()
    providers = hybrid.get_available_providers()
    
    prompt = "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå authentication log ‡πÅ‡∏•‡∏∞‡∏ö‡∏≠‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢"
    system_prompt = "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô Security Analyst"
    
    results = {}
    
    for provider in providers:
        if provider in ['local', 'openai']:
            try:
                print(f"  ‡∏ó‡∏î‡∏™‡∏≠‡∏ö {provider}...")
                start_time = time.time()
                
                response = await hybrid.generate_response(prompt, system_prompt, provider)
                duration = time.time() - start_time
                
                results[provider] = {
                    'duration': duration,
                    'response_length': len(response),
                    'success': True
                }
                
                print(f"    ‚è±Ô∏è  ‡πÄ‡∏ß‡∏•‡∏≤: {duration:.2f}s")
                print(f"    üìù ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß: {len(response)} characters")
                
            except Exception as e:
                results[provider] = {
                    'success': False,
                    'error': str(e)
                }
                print(f"    ‚ùå ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
    
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
    if len(results) > 1:
        print(f"\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:")
        fastest = min([r for r in results.values() if r.get('success')], 
                     key=lambda x: x.get('duration', float('inf')), default=None)
        if fastest:
            fastest_provider = [k for k, v in results.items() if v == fastest][0]
            print(f"  üèÉ ‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: {fastest_provider} ({fastest['duration']:.2f}s)")
    
    return len([r for r in results.values() if r.get('success')]) > 0

def test_integration_with_ai_assistant():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£ integrate ‡∏Å‡∏±‡∏ö AI Assistant service"""
    print("\nüéØ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Integration ‡∏Å‡∏±‡∏ö AI Assistant...")
    
    try:
        from app.services.ai_assistant import ai_assistant
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ AI Assistant ‡πÉ‡∏ä‡πâ hybrid client
        has_hybrid = hasattr(ai_assistant, 'hybrid_client')
        print(f"  AI Assistant ‡∏°‡∏µ hybrid client: {'‚úÖ' if has_hybrid else '‚ùå'}")
        
        if has_hybrid:
            status = ai_assistant.hybrid_client.get_status()
            print(f"  Provider ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô: {status['current_provider']}")
            print(f"  Provider ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: {status['recommended_provider']}")
        
        return has_hybrid
        
    except Exception as e:
        print(f"  ‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        return False

async def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö"""
    print("ü§ñ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö Local AI Integration")
    print("=" * 60)
    
    # ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ test ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    tests = [
        ("Ollama Connection", test_ollama_connection),
        ("Model Management", test_model_management),
        ("Text Generation", test_text_generation),
        ("Chat Completion", test_chat_completion),
        ("Hybrid AI Client", test_hybrid_ai_client),
        ("Hybrid Generation", test_hybrid_generation),
        ("Token Estimation", test_token_estimation),
        ("Performance Comparison", test_performance_comparison),
        ("AI Assistant Integration", test_integration_with_ai_assistant)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\nüß™ {test_name}")
        try:
            if asyncio.iscoroutinefunction(test_func):
                result = await test_func()
            else:
                result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"‚ùå ‡∏ó‡∏î‡∏™‡∏≠‡∏ö {test_name} ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
            results.append((test_name, False))
    
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    print("\n" + "=" * 60)
    print("üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Local AI:")
    print("=" * 60)
    
    passed = 0
    total = len(results)
    
    for test_name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{test_name:.<45} {status}")
        if result:
            passed += 1
    
    print("=" * 60)
    print(f"‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏ß‡∏°: {passed}/{total} ({passed/total*100:.1f}%)")
    
    # ‡∏Ç‡πâ‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
    print(f"\nüìã ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:")
    
    if passed == 0:
        print("‚ùå Local AI ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:")
        print("  1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Ollama service ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")
        print("  2. ‡∏£‡∏±‡∏ô: docker-compose up ollama")
        print("  3. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î model: docker exec -it auth_logs_ollama ollama pull llama3.2:3b")
    
    elif passed >= total * 0.7:
        print("‚úÖ Local AI ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà:")
        print("  1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ AI_PROVIDER=local ‡πÉ‡∏ô environment")
        print("  2. ‡πÉ‡∏ä‡πâ Local AI ‡πÅ‡∏ó‡∏ô OpenAI ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢")
        print("  3. Local AI ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö offline ‡πÅ‡∏•‡∏∞‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Å‡∏ß‡πà‡∏≤")
    
    else:
        print("‚ö†Ô∏è  Local AI ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô:")
        print("  1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Ollama")
        print("  2. ‡∏•‡∏≠‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î model ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤")
        print("  3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö memory ‡πÅ‡∏•‡∏∞ disk space")
    
    print(f"\nüí° ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:")
    print("  - Local AI ‡∏ü‡∏£‡∏µ ‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ resource ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á")
    print("  - OpenAI ‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏á‡∏¥‡∏ô ‡πÅ‡∏ï‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏π‡∏á")
    print("  - ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å provider ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥")
    print("  - ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô AI_PROVIDER ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå
    print(f"\nüîß ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå:")
    print("  # ‡∏î‡∏π models ‡∏ó‡∏µ‡πà‡∏°‡∏µ")
    print("  docker exec -it auth_logs_ollama ollama list")
    print("  ")
    print("  # ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î model ‡πÉ‡∏´‡∏°‡πà")
    print("  docker exec -it auth_logs_ollama ollama pull llama3.2:1b")
    print("  ")
    print("  # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ Local AI")
    print("  export AI_PROVIDER=local")
    print("  ")
    print("  # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ OpenAI")
    print("  export AI_PROVIDER=openai")

if __name__ == "__main__":
    asyncio.run(main()) 